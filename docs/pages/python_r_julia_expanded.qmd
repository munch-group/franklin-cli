---
title: "Python, R, and Julia for Data Science"
subtitle: "A Comprehensive Comparison Guide"
format: html
---

## Introduction

This guide provides a comprehensive comparison of Python, R, and Julia for data science tasks. Whether you're transitioning between languages or choosing which to learn, this reference shows how to accomplish common data operations in all three languages.

### Language Overview

| Feature | Python | R | Julia |
|---------|--------|---|-------|
| **Primary Use** | General purpose, Data Science, ML | Statistics, Data Analysis | Scientific Computing, Performance |
| **Learning Curve** | Moderate | Steep initially | Moderate to Steep |
| **Performance** | Good with NumPy | Moderate | Excellent |
| **Ecosystem** | Vast | Statistical focus | Growing rapidly |
| **Syntax** | Clean, readable | Functional, vectorized | Mathematical, fast |

## Setup and Libraries

::: panel-tabset
### Python

```python
# Core data science libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn import preprocessing, model_selection

# Set display options
pd.set_option('display.max_columns', 10)
pd.set_option('display.precision', 3)
```

### R

```r
# Core tidyverse and data science packages
library(tidyverse)  # dplyr, ggplot2, tidyr, etc.
library(data.table)
library(lubridate)
library(forcats)
library(stringr)

# Set options
options(scipen = 999)
options(digits = 3)
```

### Julia

```julia
# Core data science packages
using DataFrames
using CSV
using Statistics
using StatsBase
using Plots
using Random

# Set random seed
Random.seed!(42)
```
:::

## Creating Data Structures

::: panel-tabset
### Python

```python
# Lists and arrays
lst = [1, 2, 3, 4, 5]
arr = np.array([1, 2, 3, 4, 5])

# Dictionary
dict_data = {'name': ['Alice', 'Bob'], 
             'age': [25, 30]}

# DataFrame
df = pd.DataFrame({
    'id': [1, 2, 3, 4],
    'name': ['Alice', 'Bob', 'Charlie', 'David'],
    'age': [25, 30, 35, 28],
    'salary': [50000, 60000, 75000, 55000]
})

# Series
series = pd.Series([1, 2, 3], index=['a', 'b', 'c'])
```

### R

```r
# Vectors
vec <- c(1, 2, 3, 4, 5)

# List
lst <- list(name = c("Alice", "Bob"), 
            age = c(25, 30))

# Data frame (base R)
df <- data.frame(
  id = 1:4,
  name = c("Alice", "Bob", "Charlie", "David"),
  age = c(25, 30, 35, 28),
  salary = c(50000, 60000, 75000, 55000)
)

# Tibble (tidyverse)
tbl <- tibble(
  id = 1:4,
  name = c("Alice", "Bob", "Charlie", "David"),
  age = c(25, 30, 35, 28),
  salary = c(50000, 60000, 75000, 55000)
)
```

### Julia

```julia
# Arrays
arr = [1, 2, 3, 4, 5]

# Dictionary
dict_data = Dict("name" => ["Alice", "Bob"],
                 "age" => [25, 30])

# DataFrame
df = DataFrame(
    id = 1:4,
    name = ["Alice", "Bob", "Charlie", "David"],
    age = [25, 30, 35, 28],
    salary = [50000, 60000, 75000, 55000]
)

# Named tuple
nt = (name="Alice", age=25)
```
:::

## Data Import/Export

::: panel-tabset
### Python

```python
# CSV
df = pd.read_csv('data.csv')
df.to_csv('output.csv', index=False)

# Excel
df = pd.read_excel('data.xlsx', sheet_name='Sheet1')
df.to_excel('output.xlsx', index=False)

# JSON
df = pd.read_json('data.json')
df.to_json('output.json', orient='records')

# Parquet (efficient storage)
df = pd.read_parquet('data.parquet')
df.to_parquet('output.parquet')
```

### R

```r
# CSV
df <- read_csv("data.csv")  # readr package
write_csv(df, "output.csv")

# Excel
library(readxl)
df <- read_excel("data.xlsx", sheet = "Sheet1")
library(writexl)
write_xlsx(df, "output.xlsx")

# JSON
library(jsonlite)
df <- fromJSON("data.json")
write_json(df, "output.json")

# RDS (R's native format)
saveRDS(df, "data.rds")
df <- readRDS("data.rds")
```

### Julia

```julia
using CSV, XLSX, JSON, Parquet

# CSV
df = CSV.read("data.csv", DataFrame)
CSV.write("output.csv", df)

# Excel
df = DataFrame(XLSX.readtable("data.xlsx", "Sheet1"))
XLSX.writetable("output.xlsx", df)

# JSON
using JSON3
df = JSON3.read(read("data.json", String))
write("output.json", JSON3.write(df))

# Parquet
df = read_parquet("data.parquet")
write_parquet("output.parquet", df)
```
:::

## Basic Data Inspection

::: panel-tabset
### Python

```python
# Basic info
print(df.shape)          # (rows, columns)
print(df.dtypes)         # Data types
print(df.info())         # Summary info
print(df.describe())     # Statistical summary

# First/last rows
print(df.head())         # First 5 rows
print(df.tail())         # Last 5 rows

# Unique values
print(df['name'].nunique())
print(df['name'].unique())
print(df['name'].value_counts())

# Missing values
print(df.isnull().sum())
```

### R

```r
# Basic info
dim(df)                  # Dimensions
str(df)                  # Structure
glimpse(df)              # dplyr version
summary(df)              # Statistical summary

# First/last rows
head(df)                 # First 6 rows
tail(df)                 # Last 6 rows

# Unique values
n_distinct(df$name)
unique(df$name)
table(df$name)

# Missing values
sum(is.na(df))
colSums(is.na(df))
```

### Julia

```julia
# Basic info
size(df)                 # (rows, columns)
describe(df)             # Statistical summary
names(df)                # Column names
eltype.(eachcol(df))     # Column types

# First/last rows
first(df, 5)             # First 5 rows
last(df, 5)              # Last 5 rows

# Unique values
length(unique(df.name))
unique(df.name)
countmap(df.name)        # Using StatsBase

# Missing values
sum(ismissing.(df.name))
```
:::

## Selecting Data

::: panel-tabset
### Python

```python
# Select columns
df['name']                    # Single column (Series)
df[['name', 'age']]          # Multiple columns

# Select rows by position
df.iloc[0]                    # First row
df.iloc[0:3]                  # First 3 rows
df.iloc[[0, 2, 4]]           # Specific rows

# Select rows by label
df.loc[0]                     # Row with index 0
df.loc[0:2, ['name', 'age']] # Rows and columns

# Select by condition
df[df['age'] > 30]
df[(df['age'] > 25) & (df['salary'] > 55000)]
df.query('age > 25 and salary > 55000')
```

### R

```r
# Select columns
df$name                       # Single column
df[, c("name", "age")]       # Multiple columns
select(df, name, age)         # dplyr

# Select rows by position
df[1, ]                       # First row
df[1:3, ]                     # First 3 rows
slice(df, 1:3)                # dplyr

# Select by condition
df[df$age > 30, ]
filter(df, age > 30)          # dplyr
filter(df, age > 25, salary > 55000)

# Combined selection
df %>%
  filter(age > 25) %>%
  select(name, salary)
```

### Julia

```julia
# Select columns
df.name                       # Single column
df[:, [:name, :age]]         # Multiple columns
select(df, :name, :age)

# Select rows by position
df[1, :]                      # First row
df[1:3, :]                    # First 3 rows

# Select by condition
df[df.age .> 30, :]
filter(row -> row.age > 30, df)
filter(row -> row.age > 25 && row.salary > 55000, df)

# Combined selection
df |> 
  x -> filter(row -> row.age > 25, x) |>
  x -> select(x, :name, :salary)
```
:::

## Data Manipulation

::: panel-tabset
### Python

```python
# Add new column
df['bonus'] = df['salary'] * 0.1
df = df.assign(total_comp=df['salary'] + df['bonus'])

# Modify existing column
df['age'] = df['age'] + 1

# Drop columns
df = df.drop(['bonus'], axis=1)

# Rename columns
df = df.rename(columns={'name': 'employee_name'})

# Replace values
df['name'] = df['name'].replace('Alice', 'Alicia')

# Apply function
df['age_group'] = df['age'].apply(lambda x: 'Young' if x < 30 else 'Adult')
```

### R

```r
# Add new column
df$bonus <- df$salary * 0.1
df <- df %>% mutate(total_comp = salary + bonus)

# Modify existing column
df$age <- df$age + 1
df <- df %>% mutate(age = age + 1)

# Drop columns
df <- df %>% select(-bonus)

# Rename columns
df <- df %>% rename(employee_name = name)

# Replace values
df$name[df$name == "Alice"] <- "Alicia"
df <- df %>% mutate(name = if_else(name == "Alice", "Alicia", name))

# Apply function
df <- df %>% 
  mutate(age_group = if_else(age < 30, "Young", "Adult"))
```

### Julia

```julia
# Add new column
df.bonus = df.salary .* 0.1
transform!(df, :salary => (x -> x .* 0.1) => :bonus)

# Modify existing column
df.age = df.age .+ 1
transform!(df, :age => (x -> x .+ 1) => :age)

# Drop columns
select!(df, Not(:bonus))

# Rename columns
rename!(df, :name => :employee_name)

# Replace values
df.name = replace(df.name, "Alice" => "Alicia")

# Apply function
df.age_group = [x < 30 ? "Young" : "Adult" for x in df.age]
```
:::

## Grouping and Aggregation

::: panel-tabset
### Python

```python
# Simple aggregation
df.groupby('age_group')['salary'].mean()

# Multiple aggregations
agg_df = df.groupby('age_group').agg({
    'salary': ['mean', 'std', 'count'],
    'age': ['min', 'max']
}).reset_index()

# Custom aggregation
def custom_agg(x):
    return pd.Series({
        'mean_salary': x['salary'].mean(),
        'total_comp': x['salary'].sum(),
        'employee_count': len(x)
    })

result = df.groupby('age_group').apply(custom_agg)

# Transform (same size output)
df['salary_zscore'] = df.groupby('age_group')['salary'].transform(
    lambda x: (x - x.mean()) / x.std()
)
```

### R

```r
# Simple aggregation
df %>% 
  group_by(age_group) %>% 
  summarise(mean_salary = mean(salary))

# Multiple aggregations
agg_df <- df %>%
  group_by(age_group) %>%
  summarise(
    mean_salary = mean(salary),
    std_salary = sd(salary),
    count = n(),
    min_age = min(age),
    max_age = max(age)
  )

# Custom aggregation
df %>%
  group_by(age_group) %>%
  summarise(
    mean_salary = mean(salary),
    total_comp = sum(salary),
    employee_count = n(),
    .groups = 'drop'
  )

# Transform (same size output)
df <- df %>%
  group_by(age_group) %>%
  mutate(salary_zscore = (salary - mean(salary)) / sd(salary))
```

### Julia

```julia
using Statistics

# Simple aggregation
combine(groupby(df, :age_group), :salary => mean)

# Multiple aggregations
agg_df = combine(groupby(df, :age_group),
    :salary => mean => :mean_salary,
    :salary => std => :std_salary,
    nrow => :count,
    :age => minimum => :min_age,
    :age => maximum => :max_age
)

# Custom aggregation
combine(groupby(df, :age_group)) do sdf
    DataFrame(
        mean_salary = mean(sdf.salary),
        total_comp = sum(sdf.salary),
        employee_count = nrow(sdf)
    )
end

# Transform (same size output)
transform!(groupby(df, :age_group),
    :salary => (x -> (x .- mean(x)) ./ std(x)) => :salary_zscore
)
```
:::

## Joining/Merging Data

::: panel-tabset
### Python

```python
# Sample DataFrames
df1 = pd.DataFrame({'id': [1, 2, 3], 'name': ['A', 'B', 'C']})
df2 = pd.DataFrame({'id': [2, 3, 4], 'value': [10, 20, 30]})

# Inner join
inner = pd.merge(df1, df2, on='id', how='inner')

# Left join
left = pd.merge(df1, df2, on='id', how='left')

# Right join
right = pd.merge(df1, df2, on='id', how='right')

# Outer join
outer = pd.merge(df1, df2, on='id', how='outer')

# Join on multiple columns
merged = pd.merge(df1, df2, on=['id', 'date'], how='inner')

# Join with different column names
merged = pd.merge(df1, df2, left_on='id1', right_on='id2')
```

### R

```r
# Sample data frames
df1 <- tibble(id = c(1, 2, 3), name = c("A", "B", "C"))
df2 <- tibble(id = c(2, 3, 4), value = c(10, 20, 30))

# Inner join
inner <- inner_join(df1, df2, by = "id")

# Left join
left <- left_join(df1, df2, by = "id")

# Right join
right <- right_join(df1, df2, by = "id")

# Full outer join
outer <- full_join(df1, df2, by = "id")

# Join on multiple columns
merged <- inner_join(df1, df2, by = c("id", "date"))

# Join with different column names
merged <- left_join(df1, df2, by = c("id1" = "id2"))
```

### Julia

```julia
# Sample DataFrames
df1 = DataFrame(id = [1, 2, 3], name = ["A", "B", "C"])
df2 = DataFrame(id = [2, 3, 4], value = [10, 20, 30])

# Inner join
inner = innerjoin(df1, df2, on = :id)

# Left join
left = leftjoin(df1, df2, on = :id)

# Right join
right = rightjoin(df1, df2, on = :id)

# Outer join
outer = outerjoin(df1, df2, on = :id)

# Join on multiple columns
merged = innerjoin(df1, df2, on = [:id, :date])

# Join with different column names
merged = leftjoin(df1, df2, on = :id1 => :id2)
```
:::

## Reshaping Data

::: panel-tabset
### Python

```python
# Wide to long (melt)
long_df = pd.melt(df, 
                  id_vars=['id', 'name'],
                  value_vars=['value1', 'value2'],
                  var_name='variable',
                  value_name='value')

# Long to wide (pivot)
wide_df = long_df.pivot(index='id',
                        columns='variable',
                        values='value')

# Pivot table with aggregation
pivot_table = pd.pivot_table(df,
                             values='value',
                             index='category',
                             columns='year',
                             aggfunc='mean')

# Stack/unstack
stacked = df.set_index(['id', 'name']).stack()
unstacked = stacked.unstack()
```

### R

```r
library(tidyr)

# Wide to long
long_df <- df %>%
  pivot_longer(cols = c(value1, value2),
               names_to = "variable",
               values_to = "value")

# Long to wide
wide_df <- long_df %>%
  pivot_wider(names_from = variable,
              values_from = value)

# With aggregation
wide_df <- long_df %>%
  pivot_wider(names_from = variable,
              values_from = value,
              values_fn = mean)

# Separate and unite columns
df %>%
  separate(full_name, into = c("first", "last"), sep = " ") %>%
  unite(full_name, first, last, sep = "_")
```

### Julia

```julia
# Wide to long (stack)
long_df = stack(df, [:value1, :value2],
                variable_name = :variable,
                value_name = :value)

# Long to wide (unstack)
wide_df = unstack(long_df, :id, :variable, :value)

# With aggregation
wide_df = unstack(long_df, :id, :variable, :value, 
                  combine = mean)

# Custom reshaping
wide_df = combine(groupby(long_df, [:id, :variable])) do sdf
    DataFrame(value = mean(sdf.value))
end |> 
x -> unstack(x, :id, :variable, :value)
```
:::

## Missing Data Handling

::: panel-tabset
### Python

```python
# Check for missing
df.isnull().sum()
df.isna().any()

# Drop missing
df.dropna()                     # Drop rows with any NaN
df.dropna(subset=['col1'])     # Drop rows where col1 is NaN
df.dropna(axis=1)              # Drop columns with any NaN

# Fill missing
df.fillna(0)                   # Fill with constant
df.fillna(method='ffill')      # Forward fill
df.fillna(method='bfill')      # Backward fill
df.fillna(df.mean())           # Fill with mean

# Interpolate
df.interpolate(method='linear')
```

### R

```r
# Check for missing
sum(is.na(df))
colSums(is.na(df))
complete.cases(df)

# Drop missing
na.omit(df)                    # Drop rows with any NA
drop_na(df)                    # tidyr version
drop_na(df, col1)              # Drop rows where col1 is NA

# Fill missing
replace_na(df, list(col1 = 0, col2 = "Unknown"))
fill(df, col1, .direction = "down")  # Fill down
fill(df, col1, .direction = "up")    # Fill up

# Replace with mean
df %>%
  mutate(col1 = if_else(is.na(col1), mean(col1, na.rm = TRUE), col1))
```

### Julia

```julia
# Check for missing
sum(ismissing.(df.col1))
any(ismissing.(df.col1))

# Drop missing
dropmissing(df)                # Drop rows with any missing
dropmissing(df, :col1)         # Drop rows where col1 is missing

# Fill missing
coalesce.(df.col1, 0)          # Replace missing with 0
df.col1 = coalesce.(df.col1, mean(skipmissing(df.col1)))

# Forward/backward fill
function ffill!(v)
    for i in 2:length(v)
        if ismissing(v[i])
            v[i] = v[i-1]
        end
    end
end
```
:::

## String Operations

::: panel-tabset
### Python

```python
# String methods
df['name'].str.lower()          # Lowercase
df['name'].str.upper()          # Uppercase
df['name'].str.title()          # Title case
df['name'].str.strip()          # Remove whitespace
df['name'].str.len()            # String length

# Pattern matching
df['name'].str.contains('pattern')
df['name'].str.startswith('A')
df['name'].str.endswith('z')

# Extraction and replacement
df['name'].str.extract(r'(\d+)')       # Extract numbers
df['name'].str.replace('old', 'new')   # Replace
df['name'].str.split('_', expand=True) # Split into columns

# Concatenation
df['full_name'] = df['first'] + ' ' + df['last']
```

### R

```r
library(stringr)

# String functions
str_to_lower(df$name)           # Lowercase
str_to_upper(df$name)           # Uppercase
str_to_title(df$name)           # Title case
str_trim(df$name)               # Remove whitespace
str_length(df$name)             # String length

# Pattern matching
str_detect(df$name, "pattern")
str_starts(df$name, "A")
str_ends(df$name, "z")

# Extraction and replacement
str_extract(df$name, "\\d+")    # Extract numbers
str_replace(df$name, "old", "new")
str_split(df$name, "_")         # Split

# Concatenation
str_c(df$first, df$last, sep = " ")
paste(df$first, df$last)
```

### Julia

```julia
# String functions
lowercase.(df.name)              # Lowercase
uppercase.(df.name)              # Uppercase
titlecase.(df.name)              # Title case
strip.(df.name)                  # Remove whitespace
length.(df.name)                 # String length

# Pattern matching
occursin.("pattern", df.name)
startswith.(df.name, "A")
endswith.(df.name, "z")

# Extraction and replacement
match.(r"\d+", df.name)          # Extract numbers
replace.(df.name, "old" => "new")
split.(df.name, "_")             # Split

# Concatenation
df.full_name = df.first .* " " .* df.last
```
:::

## Date and Time

::: panel-tabset
### Python

```python
# Parse dates
df['date'] = pd.to_datetime(df['date_string'])
df['date'] = pd.to_datetime(df['date_string'], format='%Y-%m-%d')

# Extract components
df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month
df['day'] = df['date'].dt.day
df['weekday'] = df['date'].dt.dayofweek
df['quarter'] = df['date'].dt.quarter

# Date arithmetic
df['next_month'] = df['date'] + pd.DateOffset(months=1)
df['days_diff'] = (df['end_date'] - df['start_date']).dt.days

# Date ranges
dates = pd.date_range('2023-01-01', '2023-12-31', freq='D')
business_days = pd.bdate_range('2023-01-01', '2023-12-31')
```

### R

```r
library(lubridate)

# Parse dates
df$date <- ymd(df$date_string)
df$date <- as.Date(df$date_string, format = "%Y-%m-%d")

# Extract components
df$year <- year(df$date)
df$month <- month(df$date)
df$day <- day(df$date)
df$weekday <- wday(df$date)
df$quarter <- quarter(df$date)

# Date arithmetic
df$next_month <- df$date %m+% months(1)
df$days_diff <- as.numeric(df$end_date - df$start_date)

# Date sequences
dates <- seq(ymd("2023-01-01"), ymd("2023-12-31"), by = "day")
```

### Julia

```julia
using Dates

# Parse dates
df.date = Date.(df.date_string, "yyyy-mm-dd")

# Extract components
df.year = year.(df.date)
df.month = month.(df.date)
df.day = day.(df.date)
df.weekday = dayofweek.(df.date)
df.quarter = quarterofyear.(df.date)

# Date arithmetic
df.next_month = df.date .+ Month(1)
df.days_diff = df.end_date .- df.start_date

# Date ranges
dates = Date(2023,1,1):Day(1):Date(2023,12,31)
```
:::

## Statistical Operations

::: panel-tabset
### Python

```python
# Descriptive statistics
df.mean()
df.median()
df.std()
df.var()
df.quantile([0.25, 0.75])
df.corr()                       # Correlation matrix

# Rolling statistics
df['rolling_mean'] = df['value'].rolling(window=7).mean()
df['rolling_std'] = df['value'].rolling(window=7).std()

# Ranking
df['rank'] = df['value'].rank()
df['pct_rank'] = df['value'].rank(pct=True)

# Statistical tests
from scipy import stats
t_stat, p_value = stats.ttest_ind(group1, group2)
correlation, p_value = stats.pearsonr(x, y)
```

### R

```r
# Descriptive statistics
mean(df$value)
median(df$value)
sd(df$value)
var(df$value)
quantile(df$value, c(0.25, 0.75))
cor(df[, numeric_cols])        # Correlation matrix

# Rolling statistics (using zoo or slider)
library(zoo)
df$rolling_mean <- rollmean(df$value, 7, fill = NA)

# Ranking
df$rank <- rank(df$value)
df$pct_rank <- percent_rank(df$value)

# Statistical tests
t.test(group1, group2)
cor.test(x, y)
```

### Julia

```julia
using Statistics

# Descriptive statistics
mean(df.value)
median(df.value)
std(df.value)
var(df.value)
quantile(df.value, [0.25, 0.75])
cor(Matrix(df[:, numeric_cols]))  # Correlation matrix

# Rolling statistics
using RollingFunctions
df.rolling_mean = rollmean(df.value, 7)
df.rolling_std = rollstd(df.value, 7)

# Ranking
using StatsBase
df.rank = ordinalrank(df.value)
df.pct_rank = percentrank(df.value)

# Statistical tests
using HypothesisTests
OneSampleTTest(group1, group2)
CorrelationTest(x, y)
```
:::

## Visualization Basics

::: panel-tabset
### Python

```python
import matplotlib.pyplot as plt
import seaborn as sns

# Basic plots
plt.plot(x, y)                 # Line plot
plt.scatter(x, y)              # Scatter plot
plt.hist(data, bins=30)        # Histogram
plt.bar(categories, values)    # Bar plot

# Seaborn plots
sns.scatterplot(data=df, x='x', y='y', hue='category')
sns.boxplot(data=df, x='category', y='value')
sns.heatmap(df.corr(), annot=True)

# Pandas plotting
df.plot(kind='line')
df.plot(kind='scatter', x='col1', y='col2')
df['value'].hist()
```

### R

```r
library(ggplot2)

# Basic ggplot
ggplot(df, aes(x = x, y = y)) + 
  geom_point()                  # Scatter plot

ggplot(df, aes(x = x, y = y)) + 
  geom_line()                   # Line plot

ggplot(df, aes(x = value)) + 
  geom_histogram(bins = 30)    # Histogram

ggplot(df, aes(x = category, y = value)) + 
  geom_boxplot()                # Boxplot

# Faceting
ggplot(df, aes(x = x, y = y)) + 
  geom_point() + 
  facet_wrap(~ category)
```

### Julia

```julia
using Plots

# Basic plots
plot(x, y)                      # Line plot
scatter(x, y)                   # Scatter plot
histogram(data, bins=30)        # Histogram
bar(categories, values)         # Bar plot

# StatsPlots for DataFrames
using StatsPlots
@df df scatter(:x, :y, group=:category)
@df df boxplot(:category, :value)

# Multiple series
plot(x, [y1 y2 y3], label=["Series 1" "Series 2" "Series 3"])
```
:::

## Performance Optimization

::: panel-tabset
### Python

```python
# Vectorization
# Bad: Loop
result = []
for i in range(len(df)):
    result.append(df.iloc[i]['col1'] * 2)

# Good: Vectorized
result = df['col1'] * 2

# Use NumPy for numerical operations
np_array = df['col1'].values
result = np_array * 2

# Categorical data for memory efficiency
df['category'] = df['category'].astype('category')

# Parallel processing
from multiprocessing import Pool
with Pool(4) as p:
    results = p.map(process_function, data_chunks)
```

### R

```r
# Vectorization
# Bad: Loop
result <- c()
for(i in 1:nrow(df)) {
  result[i] <- df$col1[i] * 2
}

# Good: Vectorized
result <- df$col1 * 2

# Use data.table for speed
library(data.table)
dt <- as.data.table(df)
dt[, new_col := col1 * 2]

# Parallel processing
library(parallel)
cl <- makeCluster(4)
results <- parLapply(cl, data_chunks, process_function)
stopCluster(cl)
```

### Julia

```julia
# Julia is fast by default!
# Type stability
function process_data(x::Vector{Float64})
    return x .* 2
end

# Broadcasting
result = df.col1 .* 2

# Parallel processing
using Distributed
addprocs(4)
@distributed for i in 1:n
    process_chunk(data[i])
end

# Threads
Threads.@threads for i in 1:n
    process_chunk(data[i])
end
```
:::

## Machine Learning Basics

::: panel-tabset
### Python

```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Train model
model = LinearRegression()
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluate
mse = mean_squared_error(y_test, y_pred)
```

### R

```r
# Split data
library(caret)
train_idx <- createDataPartition(y, p = 0.8, list = FALSE)
X_train <- X[train_idx, ]
X_test <- X[-train_idx, ]
y_train <- y[train_idx]
y_test <- y[-train_idx]

# Train model
model <- lm(y ~ ., data = train_data)

# Predict
y_pred <- predict(model, newdata = test_data)

# Evaluate
mse <- mean((y_test - y_pred)^2)
```

### Julia

```julia
using MLJ

# Split data
train, test = partition(eachindex(y), 0.8, shuffle=true)
X_train, X_test = X[train, :], X[test, :]
y_train, y_test = y[train], y[test]

# Train model
LinearRegressor = @load LinearRegressor pkg=MLJLinearModels
model = LinearRegressor()
mach = machine(model, X_train, y_train)
fit!(mach)

# Predict
y_pred = predict(mach, X_test)

# Evaluate
mse = mean((y_test .- y_pred).^2)
```
:::

## Best Practices and Tips

### When to Use Each Language

**Python**
- General-purpose programming
- Machine learning and deep learning
- Web applications with data backends
- Large ecosystem of libraries

**R**
- Statistical analysis and modeling
- Academic research
- Data visualization (ggplot2)
- Statistical reporting (R Markdown)

**Julia**
- High-performance computing
- Scientific simulations
- Numerical optimization
- When Python is too slow but C is too complex

### Common Pitfalls and Solutions

| Issue | Python | R | Julia |
|-------|--------|---|-------|
| **Memory** | Use chunks, dask | Use data.table | Built-in efficiency |
| **Speed** | Use NumPy, Numba | Use Rcpp | Already fast |
| **Type Issues** | Check dtypes | Use typeof() | Type annotations |
| **Missing Data** | Use pd.NA | Use NA | Use missing |

### Package Ecosystems

| Domain | Python | R | Julia |
|--------|--------|---|-------|
| **Data Manipulation** | pandas | dplyr, data.table | DataFrames.jl |
| **Visualization** | matplotlib, seaborn | ggplot2 | Plots.jl |
| **Machine Learning** | scikit-learn | caret, mlr3 | MLJ.jl |
| **Deep Learning** | TensorFlow, PyTorch | keras, torch | Flux.jl |
| **Statistics** | scipy, statsmodels | Built-in, many packages | StatsBase.jl |

## Conclusion

Each language has its strengths:
- **Python**: Versatility and ecosystem
- **R**: Statistical depth and visualization
- **Julia**: Performance and mathematical elegance

Choose based on your specific needs, team expertise, and project requirements. Many data scientists use multiple languages, leveraging each for its strengths.

### Further Resources

**Python**
- [Pandas Documentation](https://pandas.pydata.org/)
- [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/)

**R**
- [R for Data Science](https://r4ds.had.co.nz/)
- [Advanced R](https://adv-r.hadley.nz/)

**Julia**
- [Julia Documentation](https://docs.julialang.org/)
- [Julia Data Science](https://juliadatascience.io/)

Happy coding in your language of choice! ðŸðŸ“ŠðŸ“ˆ